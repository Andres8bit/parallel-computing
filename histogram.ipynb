{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "histogram.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPTLxw98xjyNhWs/vpWiVWI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andres8bit/parallel-computing/blob/main/histogram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvyDppqU5kZP",
        "outputId": "ddb25768-833a-45fc-af4a-791655ce5dcd"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-6ac3y5se\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-6ac3y5se\n",
            "Requirement already satisfied (use --upgrade to upgrade): NVCCPlugin==0.0.2 from git+git://github.com/andreinechaev/nvcc4jupyter.git in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4308 sha256=5b40e5615f341a3dd057a84398ab258304dd8062071e54daaaa1ddb4f53bc52d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g8ny0qd1/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thEGeYMf51an",
        "outputId": "c90a2790-64b8-4485-c3e3-755b962d7da2"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3gUEv0X53vP",
        "outputId": "f44d1a58-2ed9-4ccf-dcef-5c33776d93fb"
      },
      "source": [
        "%%cu\n",
        "#include<iostream>\n",
        "\n",
        "\n",
        "__global__ void hist_blocking_kernel(unsigned char* buffer, long size,unsigned int* histo);\n",
        "void print_array(unsigned int *array, int width);\n",
        "\n",
        "int main(){\n",
        "    unsigned int host_histo[] = {0,0,0,0,0,0,0};\n",
        "    unsigned char host_buffer[] = {'p','r','o','g','r','a','m','m','i','n','g',' ','m',\n",
        "                          'a','s','s','i','v','e','l','y',' ','p','a'};\n",
        "    \n",
        "    long size = 24;\n",
        "    int buff_size = size * sizeof(char);\n",
        "    int hist_size = 7 * sizeof(int);\n",
        "\n",
        "    unsigned char *device_buffer;\n",
        "    unsigned int *device_histo;\n",
        "\n",
        "    cudaMalloc((void **) &device_buffer,buff_size);\n",
        "    cudaMalloc((void **) &device_histo,hist_size);\n",
        "\n",
        "    cudaMemcpy(device_buffer,host_buffer,buff_size,cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(device_histo,host_histo,hist_size,cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 dim_block(6);\n",
        "    dim3 dim_grid(4);\n",
        "\n",
        "    hist_blocking_kernel<<<dim_grid,dim_block>>>(device_buffer,size,device_histo); \n",
        "\n",
        "    cudaMemcpy(host_histo,device_histo,hist_size,cudaMemcpyDeviceToHost);\n",
        "\n",
        "    print_array(host_histo,7);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "void print_array(unsigned int *array, int width){\n",
        "        for(int i = 0; i < width; i++)\n",
        "        printf(\"%d\\t\", (int)array[i]);\n",
        "    \n",
        "  printf(\"\\n\");\n",
        "}\n",
        "__global__ void hist_blocking_kernel(unsigned char* buffer, long size,unsigned int* histo){\n",
        "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int section_size = (size - 1) / (blockDim.x * gridDim.x) + 1;\n",
        "    int start = i * section_size;\n",
        "    int alphabet_position =  0;\n",
        "    \n",
        "    for (int k = 0; k < section_size; k++){\n",
        "        if ( start + k < size){\n",
        "          alphabet_position = buffer[start + k] -'a';\n",
        "          if (alphabet_position >= 0 && alphabet_position < 26)\n",
        "              atomicAdd(&(histo[alphabet_position/4]), 1);\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\t3\t3\t7\t4\t1\t1\t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4otLxK2RVpjE",
        "outputId": "ad13e575-ff85-4210-dae2-d96c93c300cb"
      },
      "source": [
        "%%cu\n",
        "#include<iostream>\n",
        "\n",
        "__global__ void hist_interleaved_kernel(unsigned char* buffer,long size, \n",
        "                                        unsigned int* histo);\n",
        "                                  \n",
        "void print_array(unsigned int *array, int width);\n",
        "\n",
        "int main(){\n",
        "        unsigned int host_histo[] = {0,0,0,0,0,0,0};\n",
        "    unsigned char host_buffer[] = {'p','r','o','g','r','a','m','m','i','n','g',' ','m',\n",
        "                          'a','s','s','i','v','e','l','y',' ','p','a'};\n",
        "    \n",
        "    long size = 24;\n",
        "    int buff_size = size * sizeof(char);\n",
        "    int hist_size = 7 * sizeof(int);\n",
        "\n",
        "    unsigned char *device_buffer;\n",
        "    unsigned int *device_histo;\n",
        "\n",
        "    cudaMalloc((void **) &device_buffer,buff_size);\n",
        "    cudaMalloc((void **) &device_histo,hist_size);\n",
        "\n",
        "    cudaMemcpy(device_buffer,host_buffer,buff_size,cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(device_histo,host_histo,hist_size,cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 dim_block(6);\n",
        "    dim3 dim_grid(4);\n",
        "\n",
        "  hist_interleaved_kernel<<<dim_grid,dim_block>>>(device_buffer,size,device_histo); \n",
        "\n",
        "    cudaMemcpy(host_histo,device_histo,hist_size,cudaMemcpyDeviceToHost);\n",
        "\n",
        "    print_array(host_histo,7);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "void print_array(unsigned int *array, int width){\n",
        "        for(int i = 0; i < width; i++)\n",
        "        printf(\"%d\\t\", (int)array[i]);\n",
        "    \n",
        "  printf(\"\\n\");\n",
        "\n",
        "}\n",
        "\n",
        "__global__ void hist_interleaved_kernel(unsigned char* buffer,long size, \n",
        "                                        unsigned int* histo)\n",
        "{\n",
        "  unsigned int thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "   int alphabet_pos = 0;\n",
        "  for ( unsigned int i = thread_id; i < size; i += blockDim.x * gridDim.x){\n",
        "      alphabet_pos = buffer[i] - 'a';\n",
        "\n",
        "      if (alphabet_pos >= 0 && alphabet_pos < 26)\n",
        "          atomicAdd(&(histo[alphabet_pos/4]),1);\n",
        "  }                                             \n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\t3\t3\t7\t4\t1\t1\t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5A4JW8Kd9Ks",
        "outputId": "bb6cac95-60e7-491d-aa2c-ae6850a4414e"
      },
      "source": [
        "%%cu\n",
        "#include<iostream>\n",
        "\n",
        "__global__ void hist_privatized_kernel(unsigned char* buffer,unsigned int* histo,\n",
        "                              unsigned int num_elements, unsigned int num_bins);\n",
        "\n",
        "void print_array(unsigned int *array, int width);\n",
        "\n",
        "int main(){\n",
        "        unsigned int host_histo[] = {0,0,0,0,0,0,0};\n",
        "    unsigned char host_buffer[] = {'p','r','o','g','r','a','m','m','i','n','g',' ','m',\n",
        "                          'a','s','s','i','v','e','l','y',' ','p','a'};\n",
        "    \n",
        "    long size = 24;\n",
        "    int buff_size = size * sizeof(char);\n",
        "    int hist_size = 7 * sizeof(int);\n",
        "\n",
        "    unsigned char *device_buffer;\n",
        "    unsigned int *device_histo;\n",
        "\n",
        "    cudaMalloc((void **) &device_buffer,buff_size);\n",
        "    cudaMalloc((void **) &device_histo,hist_size);\n",
        "\n",
        "    cudaMemcpy(device_buffer,host_buffer,buff_size,cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(device_histo,host_histo,hist_size,cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 dim_block(6);\n",
        "    dim3 dim_grid(4);\n",
        "\n",
        "  hist_privatized_kernel<<<dim_grid,dim_block>>>(device_buffer,device_histo,24,7); \n",
        "\n",
        "    cudaMemcpy(host_histo,device_histo,hist_size,cudaMemcpyDeviceToHost);\n",
        "\n",
        "    print_array(host_histo,7);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "void print_array(unsigned int *array, int width){\n",
        "        for(int i = 0; i < width; i++)\n",
        "        printf(\"%d\\t\", (int)array[i]);\n",
        "    \n",
        "  printf(\"\\n\");\n",
        "\n",
        "}\n",
        "__global__ void hist_privatized_kernel(unsigned char* buffer,unsigned int* histo,\n",
        "                              unsigned int num_elements, unsigned int num_bins)\n",
        "{\n",
        "    unsigned int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    extern __shared__ unsigned int histo_s[7];\n",
        " \n",
        "    for (unsigned int binId = threadIdx.x; binId < num_bins; binId += blockDim.x)\n",
        "        histo_s[binId] = 0;\n",
        " \n",
        "    __syncthreads();\n",
        " \n",
        "    int alphabet_pos = 0;\n",
        "    for (unsigned int i = thread_id; i < num_elements; i += blockDim.x * gridDim.x)\n",
        "    {\n",
        "      alphabet_pos = buffer[i] - 'a';\n",
        "      if (alphabet_pos >= 0 && alphabet_pos < 26)\n",
        "        atomicAdd(&(histo_s[alphabet_pos/4]),1);   \n",
        "    }\n",
        " \n",
        "   __syncthreads();\n",
        " \n",
        "   for (unsigned int binId = threadIdx.x; binId < num_bins; binId += blockDim.x)\n",
        "       atomicAdd(&(histo[binId]),histo_s[binId]);\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\t3\t3\t7\t4\t1\t1\t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8WgJCzvqm3Z",
        "outputId": "fe24e643-9d36-420b-a272-a0f07b13b97e"
      },
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "\n",
        "__global__ void hist_aggr_pvt_kernel(unsigned char* buffer, unsigned int* hist \n",
        "                            ,unsigned int num_elements, unsigned int num_bins);\n",
        "void print_array(unsigned int *array, int width);\n",
        "\n",
        "int main(){\n",
        "        unsigned int host_histo[] = {0,0,0,0,0,0,0};\n",
        "    unsigned char host_buffer[] = {'p','r','o','g','r','a','m','m','i','n','g',' ','m',\n",
        "                          'a','s','s','i','v','e','l','y',' ','p','a'};\n",
        "    \n",
        "    long size = 24;\n",
        "    int buff_size = size * sizeof(char);\n",
        "    int hist_size = 7 * sizeof(int);\n",
        "\n",
        "    unsigned char *device_buffer;\n",
        "    unsigned int *device_histo;\n",
        "\n",
        "    cudaMalloc((void **) &device_buffer,buff_size);\n",
        "    cudaMalloc((void **) &device_histo,hist_size);\n",
        "\n",
        "    cudaMemcpy(device_buffer,host_buffer,buff_size,cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(device_histo,host_histo,hist_size,cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 dim_block(6);\n",
        "    dim3 dim_grid(4);\n",
        "\n",
        "  hist_aggr_pvt_kernel<<<dim_grid,dim_block>>>(device_buffer,device_histo,24,7); \n",
        "\n",
        "    cudaMemcpy(host_histo,device_histo,hist_size,cudaMemcpyDeviceToHost);\n",
        "\n",
        "    print_array(host_histo,7);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "void print_array(unsigned int *array, int width){\n",
        "        for(int i = 0; i < width; i++)\n",
        "        printf(\"%d\\t\", (int)array[i]);\n",
        "    \n",
        "  printf(\"\\n\");\n",
        "\n",
        "}\n",
        "__global__ void hist_aggr_pvt_kernel(unsigned char* buffer, unsigned int* hist \n",
        "                            ,unsigned int num_elements, unsigned int num_bins)\n",
        "{\n",
        "  unsigned int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  extern __shared__ unsigned int histo_s[7];\n",
        " \n",
        "  for (unsigned int binId = threadIdx.x; binId < num_bins; binId += blockDim.x)\n",
        "      histo_s[binId] = 0;\n",
        " \n",
        " __syncthreads();\n",
        "\n",
        " unsigned int prev = -1;\n",
        " unsigned int accumulator = 0;\n",
        " unsigned int curr = 0;\n",
        "  int pos = 0;\n",
        "\n",
        " for (unsigned int i = thread_id; i < num_elements; i += blockDim.x * gridDim.x){\n",
        "     pos = buffer[i] - 'a';\n",
        "     if ( pos >= 0 && pos < 26){\n",
        "        curr = pos/4;\n",
        "        if (curr != prev){           \n",
        "            if (accumulator > 0)\n",
        "                 atomicAdd(&(histo_s[curr]),accumulator);\n",
        "                 accumulator = 1;\n",
        "                 prev = curr;\n",
        "            }else\n",
        "                accumulator++;\n",
        "        }     \n",
        "   }\n",
        "  __syncthreads();\n",
        "  for (unsigned int binId = threadIdx.x; binId < num_bins; binId += blockDim.x)\n",
        "        atomicAdd(&(hist[binId]),histo_s[binId]);\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\t0\t0\t0\t0\t0\t0\t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}