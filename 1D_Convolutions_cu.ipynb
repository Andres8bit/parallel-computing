{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1D_Convolutions_cu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMnul2ckPsVTDyUoA9okaKt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andres8bit/parallel-computing/blob/main/1D_Convolutions_cu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZCfy1oGWa2C",
        "outputId": "baa6b4c6-0ab4-41e6-904f-1fcbc95486af"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJnniJzpX0iP",
        "outputId": "862457ba-afa3-4c0e-f1ce-939f2e0ed450"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-e7pvrs9m\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-e7pvrs9m\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4308 sha256=f1956ebee792ced57629ac50eac4be17329d6ebb22c27b75b699ce8239a20023\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jau8re1n/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFjNaJliXuiJ",
        "outputId": "dfdbc64a-fa55-4eaa-cf29-20ddc388d7e5"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCWnjdQClrKK"
      },
      "source": [
        "# The following cells implement 1D parallel convolutional alogrithims.\r\n",
        "# A Convolution is an array operation, in which the output is a wieghted sum \r\n",
        "# of nieghbor elements within the array. Here the wieghts are provided as a 1D mask.\r\n",
        "# Each implemtation uses different layers of memory  inorder to speedup computation,\r\n",
        "# by using fast local cache memory, by taking advantage that while each thread my \r\n",
        "# make calcuatlations indepently.They each use the same mask. We therefore take \r\n",
        "# advantage of L1 cache memory, \r\n",
        "# by storing our mask in a __shared__ array. This is done by the last two cells,\r\n",
        "# while the first uses does not."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8OgBrMFWtTP",
        "outputId": "82191ab2-395a-4c04-a890-4e7bb12c3734"
      },
      "source": [
        "\r\n",
        "%%cu \r\n",
        "#include <iostream> \r\n",
        "#include <math.h>\r\n",
        "#include <chrono>\r\n",
        "#include <sys/time.h>\r\n",
        "#include <time.h>\r\n",
        "\r\n",
        "#define MAX_WIDTH 5 \r\n",
        "#define WIDTH  7\r\n",
        "\r\n",
        "void print_array(float* p, int width);\r\n",
        "__global__ void conv_1D_kernel(float *input, float *mask, float *out, int mask_width, int width);\r\n",
        "\r\n",
        "\r\n",
        "int main() \r\n",
        "{\r\n",
        "    float host_in[] = {1,2,3,4,5,6,7};\r\n",
        "    float host_mask[] = {3,4,5,4,3};\r\n",
        "    float host_out[WIDTH];\r\n",
        "    float *device_in, *device_mask, *device_out;\r\n",
        "    \r\n",
        "    printf(\"input: \");\r\n",
        "    print_array(host_in, WIDTH);\r\n",
        "    printf(\"\\nmask: \");\r\n",
        "    print_array(host_mask, MAX_WIDTH);\r\n",
        " \r\n",
        "    size_t device_size = WIDTH * sizeof (float);\r\n",
        "    \r\n",
        "    cudaMalloc ((void **) &device_in, device_size);\r\n",
        "    cudaMalloc ((void **) &device_mask,MAX_WIDTH * sizeof (float));\r\n",
        "    cudaMalloc ((void **) &device_out, device_size);\r\n",
        "     \r\n",
        "    cudaMemcpy(device_in, host_in,device_size, cudaMemcpyHostToDevice);\r\n",
        "    cudaMemcpy(device_mask, host_mask, MAX_WIDTH *sizeof(float) ,cudaMemcpyHostToDevice);\r\n",
        "     \r\n",
        "    dim3 dimBlock(WIDTH);\r\n",
        "    dim3 dimGrid(32);\r\n",
        "\r\n",
        "    conv_1D_kernel<<<dimGrid, dimBlock>>> (device_in, device_mask, device_out\r\n",
        "                                             ,MAX_WIDTH,WIDTH);\r\n",
        "\r\n",
        "    cudaMemcpy(host_out, device_out,device_size, cudaMemcpyDeviceToHost);\r\n",
        " \r\n",
        "    printf(\"\\noutput: \");\r\n",
        "    print_array(host_out, WIDTH);\r\n",
        "    \r\n",
        "    cudaFree(device_in);\r\n",
        "    cudaFree(device_out);\r\n",
        "    cudaFree(device_mask);\r\n",
        " \r\n",
        "\treturn 0; \r\n",
        "} \r\n",
        "\r\n",
        "__global__ void conv_1D_kernel(float *input,float *mask, float *out, int mask_width, int width)\r\n",
        "{\r\n",
        "    \r\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;  \r\n",
        "    float temp = 0.0;\r\n",
        "    int start = i - (mask_width/2);\r\n",
        "\r\n",
        "    for(int j = 0; j < mask_width;j++)\r\n",
        "        if(start + j >= 0 && start + j < width) \r\n",
        "         temp += input[start +j] *mask[j];      \r\n",
        "    \r\n",
        "    out[i] = temp; \r\n",
        "}\r\n",
        "\r\n",
        "void print_array(float* p, int width){\r\n",
        "    for(int i = 0; i < width; i++)\r\n",
        "        printf(\"%d\\t\", (int)p[i]);\r\n",
        "    \r\n",
        "  printf(\"\\n\");\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input: 1\t2\t3\t4\t5\t6\t7\t\n",
            "\n",
            "mask: 3\t4\t5\t4\t3\t\n",
            "\n",
            "output: 22\t38\t57\t76\t95\t90\t74\t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3R3fbpeGDIK"
      },
      "source": [
        "TIled 1D Convolution kernel which uses caching and shared memory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad8ciwd1ihML",
        "outputId": "83f8b89b-efbe-4eac-b1c5-98c3b5449d08"
      },
      "source": [
        "%%cu \r\n",
        "#include <iostream> \r\n",
        "#include <math.h>\r\n",
        "#include <chrono>\r\n",
        "#include <sys/time.h>\r\n",
        "#include <time.h>\r\n",
        "\r\n",
        "#define MAX_MASK_WIDTH 10 \r\n",
        "#define WIDTH  16\r\n",
        "#define TILE_SIZE 8\r\n",
        "\r\n",
        "__constant__ float MASK[MAX_MASK_WIDTH];\r\n",
        "\r\n",
        "__global__ void tiled_conv_1D_kernel(float *input, float* out,int mask_width,int width);\r\n",
        "void print_array(float* p, int width);\r\n",
        "  \r\n",
        "int main()\r\n",
        "{ float host_in[] = {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16};\r\n",
        "  float host_out[WIDTH];\r\n",
        "  float host_mask[] = {3,4,5,4,3,2,4,5,6,7};\r\n",
        "  float *device_in, *device_out, *device_mask;\r\n",
        "  \r\n",
        "  size_t array_size = WIDTH * sizeof(float);\r\n",
        "  size_t mask_size = MAX_MASK_WIDTH * sizeof(float);\r\n",
        " \r\n",
        "  cudaMalloc((void **) &device_in,array_size);\r\n",
        "  cudaMalloc((void **) &device_out,array_size);\r\n",
        "  cudaMalloc((void **) &device_mask,mask_size);\r\n",
        "\r\n",
        "  cudaMemcpy(device_in,host_in,array_size,cudaMemcpyHostToDevice);\r\n",
        "  cudaMemcpyToSymbol(MASK,host_mask,mask_size);\r\n",
        "  \r\n",
        "  dim3 dim_block(4);\r\n",
        "  dim3 dim_grid(4);\r\n",
        "\r\n",
        "  tiled_conv_1D_kernel<<<dim_grid,dim_block>>>(device_in,device_out,MAX_MASK_WIDTH,WIDTH);\r\n",
        "  cudaMemcpy(host_out,device_out,array_size,cudaMemcpyDeviceToHost);\r\n",
        " \r\n",
        "\r\n",
        "  printf(\"\\ninput:\");\r\n",
        "  print_array(host_in,WIDTH);\r\n",
        " \r\n",
        "  printf(\"\\nmask:\");\r\n",
        "  print_array(host_mask,MAX_MASK_WIDTH);\r\n",
        " \r\n",
        "  printf(\"\\noutput\");\r\n",
        "  print_array(host_out,WIDTH);\r\n",
        "  cudaFree(device_in);\r\n",
        "  cudaFree(device_out);\r\n",
        "  cudaFree(device_mask);\r\n",
        " \r\n",
        " \r\n",
        "  return 0;\r\n",
        "}\r\n",
        "\r\n",
        "void print_array(float* p, int width){\r\n",
        "    for(int i = 0; i < width; i++)\r\n",
        "        printf(\"%d\\t\", (int)p[i]); \r\n",
        "    \r\n",
        "  printf(\"\\n\");\r\n",
        "}\r\n",
        "\r\n",
        "__global__ void tiled_conv_1D_kernel(float *input,float* out,int mask_width,int width)\r\n",
        "{\r\n",
        "  int i = blockIdx.x * blockDim.x + threadIdx.x;\r\n",
        "  __shared__ float  tile[TILE_SIZE + MAX_MASK_WIDTH - 1];\r\n",
        "  int n = mask_width/2;  \r\n",
        "  int l_halo_index = (blockIdx.x - 1) * blockDim.x + threadIdx.x;\r\n",
        "\r\n",
        "  if (threadIdx.x -(blockDim.x - n) >= 0){\r\n",
        "       tile[threadIdx.x - (blockDim.x - n)] =  (l_halo_index < 0) ? 0: input[l_halo_index];\r\n",
        "\r\n",
        "   }\r\n",
        "  tile[n + blockDim.x + threadIdx.x] = input[i];\r\n",
        "  \r\n",
        "  int r_halo_index = (blockIdx.x + 1) * blockDim.x + threadIdx.x;\r\n",
        "  if (threadIdx.x < n){\r\n",
        "     tile[n + blockDim.x + threadIdx.x] = \r\n",
        "                                (r_halo_index >= width) ? 0 : input[r_halo_index];\r\n",
        "  }\r\n",
        "  __syncthreads();\r\n",
        "\r\n",
        "  float temp = 0;\r\n",
        "  for(int j = 0; j < mask_width;j++){\r\n",
        "    temp += tile[threadIdx.x + j] * MASK[j];\r\n",
        "  }\r\n",
        "\r\n",
        "  out[i] =  temp;                                     \r\n",
        "}\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "input:0\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\t13\t14\t15\t\n",
            "\n",
            "mask:3\t4\t5\t4\t3\t2\t4\t5\t6\t7\t\n",
            "\n",
            "output28\t59\t92\t126\t78\t137\t190\t232\t170\t253\t310\t348\t150\t154\t122\t74\t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf2L3yLiKzTO"
      },
      "source": [
        "A simpler version of a tiled cached 1D convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ_qcp--LQjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f18039f-b9f8-4560-cbeb-662ebef038a4"
      },
      "source": [
        "%%cu\r\n",
        "#include <iostream>\r\n",
        "\r\n",
        "#define MAX_MASK_WIDTH 10 \r\n",
        "#define WIDTH  16\r\n",
        "#define TILE_SIZE 8\r\n",
        "\r\n",
        "__constant__ float MASK[MAX_MASK_WIDTH];\r\n",
        "__global__ void tiled_cache_conv1D_kernel(float* input,float* output, int mask_width, int width);\r\n",
        "\r\n",
        "void print_array(float* p, int width);\r\n",
        "  \r\n",
        "int main()\r\n",
        "{ float host_in[] = {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16};\r\n",
        "  float host_out[WIDTH];\r\n",
        "  float host_mask[] = {3,4,5,4,3,2,4,5,6,7};\r\n",
        "  float *device_in, *device_out, *device_mask;\r\n",
        "  \r\n",
        "  size_t array_size = WIDTH * sizeof(float);\r\n",
        "  size_t mask_size = MAX_MASK_WIDTH * sizeof(float);\r\n",
        " \r\n",
        "  cudaMalloc((void **) &device_in,array_size);\r\n",
        "  cudaMalloc((void **) &device_out,array_size);\r\n",
        "  cudaMalloc((void **) &device_mask,mask_size);\r\n",
        "\r\n",
        "  cudaMemcpy(device_in,host_in,array_size,cudaMemcpyHostToDevice);\r\n",
        "  cudaMemcpyToSymbol(MASK,host_mask,mask_size);\r\n",
        "  \r\n",
        "  dim3 dim_block(4);\r\n",
        "  dim3 dim_grid(4);\r\n",
        "\r\n",
        "  tiled_cache_conv1D_kernel<<<dim_grid,dim_block>>>(device_in,device_out,MAX_MASK_WIDTH,WIDTH);\r\n",
        "  cudaMemcpy(host_out,device_out,array_size,cudaMemcpyDeviceToHost);\r\n",
        " \r\n",
        "\r\n",
        "  printf(\"\\ninput:\");\r\n",
        "  print_array(host_in,WIDTH);\r\n",
        " \r\n",
        "  printf(\"\\nmask:\");\r\n",
        "  print_array(host_mask,MAX_MASK_WIDTH);\r\n",
        " \r\n",
        "  printf(\"\\noutput\");\r\n",
        "  print_array(host_out,WIDTH);\r\n",
        "  cudaFree(device_in);\r\n",
        "  cudaFree(device_out);\r\n",
        "  cudaFree(device_mask);\r\n",
        " \r\n",
        " \r\n",
        "  return 0;\r\n",
        "}\r\n",
        "\r\n",
        "void print_array(float* p, int width){\r\n",
        "    for(int i = 0; i < width; i++)\r\n",
        "        printf(\"%d\\t\", (int)p[i]); \r\n",
        "    \r\n",
        "  printf(\"\\n\");\r\n",
        "}\r\n",
        "__global__ void tiled_cache_conv1D_kernel(float* input,float* output, int mask_width, int width)\r\n",
        "{\r\n",
        "  int i = blockIdx.x*blockDim.x + threadIdx.x;\r\n",
        "  __shared__ float tile[TILE_SIZE];  \r\n",
        "\r\n",
        "  __syncthreads();\r\n",
        "  int cur_start = blockIdx.x * blockDim.x;\r\n",
        "  int next_start = (blockIdx.x +1) * blockDim.x;\r\n",
        "  int start = i - (mask_width/2);\r\n",
        "  int index = 0;\r\n",
        "  float temp = 0;\r\n",
        "  \r\n",
        "  for (int j = 0; j < mask_width;j++){\r\n",
        "    index = start + j;\r\n",
        "    if(index >= 0 && index < width){\r\n",
        "        if (index >= cur_start && index < next_start){\r\n",
        "            temp += tile[threadIdx.x + j - (mask_width/2)] * MASK[j];\r\n",
        "        }else{\r\n",
        "            temp +=input[index] * MASK[j];\r\n",
        "        }\r\n",
        "    }\r\n",
        "  } \r\n",
        "    output[i] = temp;\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "input:0\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\t13\t14\t15\t\n",
            "\n",
            "mask:3\t4\t5\t4\t3\t2\t4\t5\t6\t7\t\n",
            "\n",
            "output28\t59\t92\t126\t78\t137\t190\t232\t179\t253\t310\t348\t171\t154\t122\t74\t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}